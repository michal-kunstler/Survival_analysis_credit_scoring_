{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da50d037-09d6-4486-aaaf-27d170aec87c",
   "metadata": {},
   "source": [
    "# General setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be3fcb84-0f06-43a7-ad29-bed83b337f83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sksurv.metrics import integrated_brier_score, cumulative_dynamic_auc, concordance_index_ipcw\n",
    "\n",
    "from statsmodels.miscmodels.ordinal_model import OrderedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa9a0cd1-04cf-44b5-adb6-67fdf265ffe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12613, 55), (5409, 55), (12613,), (5409,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data\n",
    "test = pd.read_csv(\"test_fin.csv\", index_col=0).fillna(0)\n",
    "train = pd.read_csv(\"train_fin.csv\", index_col=0).fillna(0)\n",
    "val = pd.read_csv(\"val_fin.csv\", index_col=0).fillna(0)\n",
    "\n",
    "# Define the dictionary with the mapping\n",
    "mapping = {1: False, 2: True}\n",
    "\n",
    "# Apply the mapping to the 'col1' column\n",
    "train['status'] = train['status'].replace(mapping)\n",
    "test['status'] = test['status'].replace(mapping)\n",
    "val['status'] = val['status'].replace(mapping)\n",
    "\n",
    "y_train = np.array(\n",
    "    list(zip(train['status'], train['time_to_event'])),\n",
    "    dtype=[('status', '?'), ('time_to_event', 'double')]\n",
    ")\n",
    "\n",
    "y_test = np.array(\n",
    "    list(zip(test['status'], test['time_to_event'])),\n",
    "    dtype=[('status', '?'), ('time_to_event', 'double')]\n",
    ")\n",
    "\n",
    "times_data = np.arange(1, 70).tolist()\n",
    "train.shape, test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0bb9e3d-f603-4535-8127-d435792a7eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['time_to_event_CAT'] = np.where(train['status']== True, train['time_to_event'], -train['time_to_event'])\n",
    "test['time_to_event_CAT'] = np.where(test['status']== True, test['time_to_event'], -test['time_to_event'])\n",
    "val['time_to_event_CAT'] = np.where(val['status']== True, val['time_to_event'], -val['time_to_event'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56ae7b79-79e1-4efa-8f14-44862f131289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluation metrics definition \n",
    "    \n",
    "def calc_metrics(y_train, y_test, y_pred, times):\n",
    "\n",
    "    metrics = {\n",
    "        \"cumulative_dynamic_auc\": cumulative_dynamic_auc(y_train, y_test, y_pred, times),\n",
    "        \"concordance_index_ipcw\": concordance_index_ipcw(y_train, y_test, y_pred)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def display_metric(metrics):\n",
    "    print(metrics[\"cumulative_dynamic_auc\"])\n",
    "    print(metrics[\"concordance_index_ipcw\"])\n",
    "\n",
    "#y_train = np.array([(1, 3), (0, 9) , (1, 0), (1, 8), (0, 9), (1,1)])\n",
    "#y_test = np.array([(0, 3), (1, 9) , (0, 0), (1, 8), (0, 9), (1,1)])\n",
    "#y_pred = np.array([(1, 3), (0, 2) , (1, 1), (1, 8), (0, 9), (1,1)])\n",
    "\n",
    "#print(calc_metrics(y_train, y_test, y_pred, times)[\"classification_report\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8043ae3-67c2-4532-8db8-58991e04f876",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9ad6242d-4be9-41a5-9f6d-04480097ce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['CHANNEL.B', 'CHANNEL.C', 'CHANNEL.R', 'NUM_BO.1', 'NUM_BO.2',\n",
    "       'NUM_BO.3', 'NUM_BO.4', 'FIRST_FLAG.N', 'FIRST_FLAG.Y', 'PURPOSE.C',\n",
    "       'PURPOSE.P', 'PURPOSE.R', 'PROP.CO', 'PROP.CP', 'PROP.MH', 'PROP.PU',\n",
    "       'PROP.SF', 'OCC_STAT.I', 'OCC_STAT.P', 'OCC_STAT.S',\n",
    "       'HIGH_BALANCE_LOAN_INDICATOR.N', 'HIGH_BALANCE_LOAN_INDICATOR.Y',\n",
    "       'ORIG_RATE', 'CURR_RATE', 'ORIG_UPB', 'ORIG_TERM', 'OLTV', 'OCLTV',\n",
    "       'DTI', 'CSCORE_B', 'CSCORE_C', 'MI_PCT', 'NO_UNITS.1', 'NO_UNITS.2',\n",
    "       'NO_UNITS.3', 'NO_UNITS.4', 'MI_TYPE.BPMI', 'MI_TYPE.LPMI',\n",
    "       'MI_TYPE.None', 'max_deliq_6m',\n",
    "       'max_deliq_12m', 'max_deliq_6_12', 'if_deliq', 'deliq_stat_6m',\n",
    "       'deliq_stat_12m', 'deliq_stat_avg_3_12', 'deliq_stat_avg_6_12',\n",
    "       'ACT_PERIOD', 'CURRENT_UPB', 'LOAN_AGE',\n",
    "       'ADJ_REM_MONTHS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d1b2575-9868-40f0-99de-7705b6cc2bea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "elapsed:  14.032626867294312\n"
     ]
    }
   ],
   "source": [
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sklearn.model_selection import ShuffleSplit, RandomizedSearchCV\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "estimator = RandomSurvivalForest()\n",
    "\n",
    "parameter_space = {\n",
    "     'max_depth': [2, 3, 4, 5, 8],\n",
    "     'max_features': ['log2', 'sqrt'],\n",
    "     'min_samples_leaf': [1, 2, 4],\n",
    "     'min_samples_split': [2, 5, 10],\n",
    "     'n_estimators': [10, 25, 50, 75, 100, 200]\n",
    "}\n",
    "\n",
    "cross_validation_schema = ShuffleSplit(n_splits=3)\n",
    "\n",
    "n_iterations = 1\n",
    "\n",
    "random_search_cv_1 = RandomizedSearchCV(\n",
    "    estimator=estimator,\n",
    "    param_distributions=parameter_space,\n",
    "    n_iter=n_iterations,\n",
    "    n_jobs=-1,\n",
    "    cv=cross_validation_schema,\n",
    "    random_state=123,\n",
    "    return_train_score=False,\n",
    "    verbose=3 #Controls the verbosity: the higher, the more messages.\n",
    "\n",
    ")\n",
    "rf_search_1 = random_search_cv_1.fit(train[features], y_train)\n",
    "\n",
    "end = time.time()\n",
    "print(\"elapsed: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "28b1d819-6ea5-48d8-b447-1e0d72d623a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def RF_survival(train, test, features, y_train, params):\n",
    "    clf = RandomSurvivalForest(**params)\n",
    "    clf.fit(train[features], y_train)\n",
    "    preds = clf.predict(test[features])\n",
    "    return preds, clf\n",
    "\n",
    "def RF_survival_result_wrapper(train, test, features, y_train, y_test, times, verbose, params):\n",
    "    model_output, model = RF_survival(\n",
    "        train, test, features, y_train, params\n",
    "    )\n",
    "    results = calc_metrics(y_train, y_test, model_output, times)\n",
    "    return results, model_output, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e0a53359-6800-461f-8b8d-d1f57746444f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cumulative_dynamic_auc': (array([0.9640639 , 0.93039611, 0.93334606, 0.92611015, 0.92633715,\n",
      "       0.93003311, 0.93269675, 0.93527036, 0.93419775, 0.93476598,\n",
      "       0.93645336, 0.93725814, 0.9375644 , 0.93815811, 0.93811723,\n",
      "       0.93966345, 0.94153021, 0.9410524 , 0.94354454, 0.94157556,\n",
      "       0.93913982, 0.93999631, 0.94088523, 0.94304688, 0.9460297 ,\n",
      "       0.9483384 , 0.95003933, 0.95131511, 0.95216283, 0.95325806,\n",
      "       0.95400699, 0.95579434, 0.9578193 , 0.9568102 , 0.95776264,\n",
      "       0.95949324, 0.9602784 , 0.96208382, 0.96331747, 0.9655946 ,\n",
      "       0.96780712, 0.96567501, 0.9653051 , 0.96773053, 0.96811923,\n",
      "       0.96790248, 0.96875167, 0.96843004, 0.969144  , 0.95031173,\n",
      "       0.94298763, 0.94148834, 0.94345655, 0.94165075, 0.94274767,\n",
      "       0.94223145, 0.94303265, 0.94382248, 0.94395173, 0.9436104 ,\n",
      "       0.94552136, 0.94221778, 0.94185208, 0.94343782, 0.9448899 ,\n",
      "       0.94496821, 0.94842136, 0.94954129, 0.95105384]), 0.947419407318438), 'concordance_index_ipcw': (0.893009087795908, 2290787, 202305, 52, 44876)}\n",
      "[20.61655228  0.92529518  0.2585392  ... 38.52237171  0.29617414\n",
      "  0.95334325]\n"
     ]
    }
   ],
   "source": [
    "results_RF, output, RF_model = RF_survival_result_wrapper(\n",
    "    train,\n",
    "    test,\n",
    "    features,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    times_data,\n",
    "    False,\n",
    "    rf_search_1.best_params_\n",
    ")\n",
    "\n",
    "print(results_RF)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e13c3350-1d4b-4af9-a45b-2ae6471f7b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 10,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 8}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END max_depth=8, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=10;, score=0.896 total time=   3.7s\n",
      "[CV 1/3] END max_depth=8, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=10;, score=0.904 total time=   5.1s\n",
      "[CV 3/3] END max_depth=8, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=10;, score=0.924 total time=   5.8s\n"
     ]
    }
   ],
   "source": [
    "rf_search_1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e7fa723f-2ebd-4c5f-9420-b5263acc9245",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidModelError",
     "evalue": "Model type not yet supported by TreeExplainer: <class 'sksurv.ensemble.forest.RandomSurvivalForest'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidModelError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [85]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTreeExplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRF_model\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshap_values(test[features])\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/virt/lib/python3.8/site-packages/shap/explainers/_tree.py:149\u001b[0m, in \u001b[0;36mTree.__init__\u001b[0;34m(self, model, data, model_output, feature_perturbation, feature_names, approximate, **deprecated_options)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_perturbation \u001b[38;5;241m=\u001b[39m feature_perturbation\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpected_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mTreeEnsemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_missing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_output \u001b[38;5;241m=\u001b[39m model_output\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m#self.model_output = self.model.model_output # this allows the TreeEnsemble to translate model outputs types by how it loads the model\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/virt/lib/python3.8/site-packages/shap/explainers/_tree.py:993\u001b[0m, in \u001b[0;36mTreeEnsemble.__init__\u001b[0;34m(self, model, data, data_missing, model_output)\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_offset \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minit_params[param_idx]\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 993\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidModelError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type not yet supported by TreeExplainer: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(model)))\n\u001b[1;32m    995\u001b[0m \u001b[38;5;66;03m# build a dense numpy version of all the tree objects\u001b[39;00m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrees \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrees:\n",
      "\u001b[0;31mInvalidModelError\u001b[0m: Model type not yet supported by TreeExplainer: <class 'sksurv.ensemble.forest.RandomSurvivalForest'>"
     ]
    }
   ],
   "source": [
    "shap_values = shap.TreeExplainer(RF_model).shap_values(test[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67dde93-a05f-4471-b7f3-0bcc31e9b40d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52487216-5012-4733-aa53-fdd43ba37aee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fbb5796-7a58-4b2b-87f1-a5f151b7928d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Gradient Boosted Cox model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1787764e-d0cb-4d03-bfa9-c78345daf119",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['CHANNEL.B', 'CHANNEL.C', 'CHANNEL.R', 'NUM_BO.1', 'NUM_BO.2',\n",
    "       'NUM_BO.3', 'NUM_BO.4', 'FIRST_FLAG.N', 'FIRST_FLAG.Y', 'PURPOSE.C',\n",
    "       'PURPOSE.P', 'PURPOSE.R', 'PROP.CO', 'PROP.CP', 'PROP.MH', 'PROP.PU',\n",
    "       'PROP.SF', 'OCC_STAT.I', 'OCC_STAT.P', 'OCC_STAT.S',\n",
    "       'HIGH_BALANCE_LOAN_INDICATOR.N', 'HIGH_BALANCE_LOAN_INDICATOR.Y',\n",
    "       'ORIG_RATE', 'CURR_RATE', 'ORIG_UPB', 'ORIG_TERM', 'OLTV', 'OCLTV',\n",
    "       'DTI', 'CSCORE_B', 'CSCORE_C', 'MI_PCT', 'NO_UNITS.1', 'NO_UNITS.2',\n",
    "       'NO_UNITS.3', 'NO_UNITS.4', 'MI_TYPE.BPMI', 'MI_TYPE.LPMI',\n",
    "       'MI_TYPE.None', 'max_deliq_6m',\n",
    "       'max_deliq_12m', 'max_deliq_6_12', 'if_deliq', 'deliq_stat_6m',\n",
    "       'deliq_stat_12m', 'deliq_stat_avg_3_12', 'deliq_stat_avg_6_12',\n",
    "       'ACT_PERIOD', 'CURRENT_UPB', 'LOAN_AGE',\n",
    "       'ADJ_REM_MONTHS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cf12c4a-1ac4-4060-b354-33a0ab4275bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "elapsed:  2231.154829978943\n",
      "{'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 8, 'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis\n",
    "\n",
    "start = time.time()\n",
    "estimator = GradientBoostingSurvivalAnalysis()\n",
    "\n",
    "parameter_space = {\n",
    "     'max_depth': [2, 3, 4, 5, 8],\n",
    "     'max_features': ['log2', 'sqrt'],\n",
    "     'min_samples_leaf': [1, 2, 4],\n",
    "     'min_samples_split': [2, 5, 10],\n",
    "     'n_estimators': [10, 25, 50, 75, 100, 200],\n",
    "     'learning_rate': [0.1, 0.01, 0.005, 0.001]\n",
    "}\n",
    "\n",
    "cross_validation_schema = ShuffleSplit(n_splits=3)\n",
    "\n",
    "n_iterations = 30\n",
    "\n",
    "random_search_cv_1 = RandomizedSearchCV(\n",
    "    estimator=estimator,\n",
    "    param_distributions=parameter_space,\n",
    "    n_iter=n_iterations,\n",
    "    n_jobs=-1,\n",
    "    cv=cross_validation_schema,\n",
    "    random_state=123,\n",
    "    return_train_score=False,\n",
    "    verbose=3 #Controls the verbosity: the higher, the more messages.\n",
    "\n",
    ")\n",
    "GBC_search_1 = random_search_cv_1.fit(train[features], y_train)\n",
    "\n",
    "end = time.time()\n",
    "print(\"elapsed: \", end - start)\n",
    "\n",
    "print(GBC_search_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c606c236-943d-41d6-87ed-0a01f5fa625d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GBC_survival(train, test, features, y_train, params):\n",
    "    clf = GradientBoostingSurvivalAnalysis(**params)\n",
    "    clf.fit(train[features], y_train)\n",
    "    preds = clf.predict(test[features])\n",
    "    return preds, clf\n",
    "\n",
    "def GBC_survival_result_wrapper(train, test, features, y_train, y_test, times, verbose, params):\n",
    "    model_output, model = GBC_survival(\n",
    "        train, test, features, y_train, params\n",
    "    )\n",
    "    results = calc_metrics(y_train, y_test, model_output, times)\n",
    "    return results, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5f64e2e-0b22-4298-9f7d-8ff8729db9c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cumulative_dynamic_auc': (array([0.97623493, 0.95796109, 0.96067957, 0.95349818, 0.95278489,\n",
      "       0.95624002, 0.95843844, 0.9609077 , 0.9602552 , 0.96038156,\n",
      "       0.9610007 , 0.96210104, 0.96222434, 0.96239795, 0.96232456,\n",
      "       0.96362271, 0.96506972, 0.96459935, 0.96597508, 0.96252426,\n",
      "       0.95965822, 0.96015873, 0.9610724 , 0.9630192 , 0.96528994,\n",
      "       0.96737988, 0.96805581, 0.96889492, 0.9689586 , 0.96973764,\n",
      "       0.97080505, 0.97209988, 0.97365071, 0.97282275, 0.97301358,\n",
      "       0.97416397, 0.974929  , 0.9763289 , 0.97724717, 0.97854026,\n",
      "       0.97959263, 0.97665705, 0.97637102, 0.97801468, 0.97795184,\n",
      "       0.97784057, 0.97832496, 0.97822002, 0.97864464, 0.96204088,\n",
      "       0.95488753, 0.95218367, 0.95348005, 0.9505172 , 0.95184253,\n",
      "       0.9514788 , 0.95227654, 0.95301781, 0.95245862, 0.95178278,\n",
      "       0.95354485, 0.95160006, 0.94989032, 0.9510048 , 0.9518676 ,\n",
      "       0.95263401, 0.95491149, 0.95602238, 0.95713518]), 0.9650253653619675), 'concordance_index_ipcw': (0.9159038752947022, 2349918, 143224, 2, 44876)}\n"
     ]
    }
   ],
   "source": [
    "results_GBC, model = GBC_survival_result_wrapper(\n",
    "    train,\n",
    "    test,\n",
    "    features,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    times_data,\n",
    "    False,\n",
    "    GBC_search_1.best_params_\n",
    ")\n",
    "\n",
    "print(results_GBC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f4b3ab-3d32-4f0b-b41f-1c8eaec46f14",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96f40fc9-3b5a-4230-a148-51fd73717f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['CHANNEL.B', 'CHANNEL.C', 'CHANNEL.R', 'NUM_BO.1', 'NUM_BO.2',\n",
    "       'NUM_BO.3', 'NUM_BO.4', 'FIRST_FLAG.N', 'FIRST_FLAG.Y', 'PURPOSE.C',\n",
    "       'PURPOSE.P', 'PURPOSE.R', 'PROP.CO', 'PROP.CP', 'PROP.MH', 'PROP.PU',\n",
    "       'PROP.SF', 'OCC_STAT.I', 'OCC_STAT.P', 'OCC_STAT.S',\n",
    "       'HIGH_BALANCE_LOAN_INDICATOR.N', 'HIGH_BALANCE_LOAN_INDICATOR.Y',\n",
    "       'ORIG_RATE', 'CURR_RATE', 'ORIG_UPB', 'ORIG_TERM', 'OLTV', 'OCLTV',\n",
    "       'DTI', 'CSCORE_B', 'CSCORE_C', 'MI_PCT', 'NO_UNITS.1', 'NO_UNITS.2',\n",
    "       'NO_UNITS.3', 'NO_UNITS.4', 'MI_TYPE.BPMI', 'MI_TYPE.LPMI',\n",
    "       'MI_TYPE.None', 'max_deliq_6m',\n",
    "       'max_deliq_12m', 'max_deliq_6_12', 'if_deliq', 'deliq_stat_6m',\n",
    "       'deliq_stat_12m', 'deliq_stat_avg_3_12', 'deliq_stat_avg_6_12',\n",
    "       'ACT_PERIOD', 'CURRENT_UPB', 'LOAN_AGE',\n",
    "       'ADJ_REM_MONTHS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ede1a9e5-3cf3-4d50-997c-1ccd4fc827db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_standardized = pd.DataFrame(scaler.fit_transform(train[features]))\n",
    "test_standardized = pd.DataFrame(scaler.transform(test[features]))\n",
    "\n",
    "train_standardized.columns = features\n",
    "test_standardized.columns = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adda4965-76e9-44a4-9c19-2060ee4935f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    }
   ],
   "source": [
    "from sksurv.svm import FastKernelSurvivalSVM\n",
    "import time\n",
    "from sklearn.model_selection import ShuffleSplit, RandomizedSearchCV\n",
    "start = time.time()\n",
    "estimator = FastKernelSurvivalSVM()\n",
    "\n",
    "parameter_space = {\n",
    "     'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "     'alpha': [0.1, 0.5, 1],\n",
    "     'gamma': [None, 0.1, 0.5, 1]\n",
    "}\n",
    "\n",
    "cross_validation_schema = ShuffleSplit(n_splits=3)\n",
    "\n",
    "n_iterations = 30\n",
    "\n",
    "random_search_cv_1 = RandomizedSearchCV(\n",
    "    estimator=estimator,\n",
    "    param_distributions=parameter_space,\n",
    "    n_iter=n_iterations,\n",
    "    n_jobs=-1,\n",
    "    cv=cross_validation_schema,\n",
    "    random_state=123,\n",
    "    return_train_score=False,\n",
    "    verbose=3 #Controls the verbosity: the higher, the more messages.\n",
    "\n",
    ")\n",
    "SVM_search_1 = random_search_cv_1.fit(train_standardized[features], y_train)\n",
    "\n",
    "end = time.time()\n",
    "print(\"elapsed: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dd802c-aa70-492b-8c2e-b134a3301c1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SVM_survival(train, test, features, y_train, params):\n",
    "    clf = FastKernelSurvivalSVM(**params)\n",
    "    clf.fit(train[features], y_train)\n",
    "    preds = clf.predict(test[features])\n",
    "    return preds\n",
    "\n",
    "def SVM_survival_result_wrapper(train, test, features, y_train, y_test, times, verbose, params):\n",
    "    model_output = SVM_survival(\n",
    "        train, test, features, y_train, params\n",
    "    )\n",
    "    results = calc_metrics(y_train, y_test, model_output, times)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "2dd3384b-680c-4dd2-a659-1706a558d679",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dc/k7crvlf16t786rmzsdp6xqs00000gn/T/ipykernel_62589/3417522465.py:3: ConvergenceWarning: Optimization did not converge: Warning: Maximum number of iterations has been exceeded.\n",
      "  clf.fit(train[features], y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cumulative_dynamic_auc': (array([0.97585663, 0.96238065, 0.96441039, 0.9610304 , 0.96057529,\n",
      "       0.96323312, 0.96437301, 0.9672231 , 0.96822594, 0.9686265 ,\n",
      "       0.96813308, 0.9689092 , 0.96924969, 0.97024592, 0.9706812 ,\n",
      "       0.97195384, 0.97332665, 0.97327144, 0.97410493, 0.97238714,\n",
      "       0.97047228, 0.97135892, 0.9721961 , 0.97332455, 0.97490007,\n",
      "       0.97597246, 0.97594646, 0.97723588, 0.97838273, 0.97849575,\n",
      "       0.9782466 , 0.97893111, 0.97996582, 0.98019293, 0.98096562,\n",
      "       0.98211741, 0.98246959, 0.98308681, 0.9841747 , 0.98518385,\n",
      "       0.98633482, 0.98578229, 0.98502034, 0.98639046, 0.9863326 ,\n",
      "       0.98583082, 0.98661272, 0.98615624, 0.98394963, 0.9723921 ,\n",
      "       0.96746869, 0.96505313, 0.96556142, 0.96244674, 0.96332916,\n",
      "       0.96313919, 0.96384784, 0.96363388, 0.96239836, 0.96230222,\n",
      "       0.96301079, 0.95975294, 0.96011914, 0.96113848, 0.96178202,\n",
      "       0.9616722 , 0.9631701 , 0.9636585 , 0.96390657]), 0.9725057189797467), 'concordance_index_ipcw': (0.9298120592579828, 2370664, 122480, 0, 44876)}\n",
      "[CV 2/3] END ............alpha=0.5, kernel=poly;, score=0.880 total time=  12.5s\n",
      "[CV 3/3] END ............alpha=0.5, kernel=poly;, score=0.913 total time=  13.3s\n",
      "[CV 2/3] END .alpha=0.5, gamma=None, kernel=rbf;, score=0.943 total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/virt/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:686: ConvergenceWarning: Optimization did not converge: Warning: Maximum number of iterations has been exceeded.\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END .alpha=0.5, gamma=None, kernel=rbf;, score=0.953 total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/virt/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:686: ConvergenceWarning: Optimization did not converge: Warning: Maximum number of iterations has been exceeded.\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END ............alpha=0.5, kernel=poly;, score=0.892 total time=  10.7s\n",
      "[CV 3/3] END .alpha=0.5, gamma=None, kernel=rbf;, score=0.940 total time= 1.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/virt/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:686: ConvergenceWarning: Optimization did not converge: Warning: Maximum number of iterations has been exceeded.\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    }
   ],
   "source": [
    "results_SVM = SVM_survival_result_wrapper(\n",
    "    train_standardized,\n",
    "    test_standardized,\n",
    "    features,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    times_data,\n",
    "    False,\n",
    "    SVM_search_1.best_params_\n",
    ")\n",
    "\n",
    "print(results_SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d62058-7eac-46f2-bfa6-dadd04ec3950",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CoxPH - uważać czy full MLE czy partial MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "26e1841e-fc92-4a61-8beb-dc3db1cfdaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['CHANNEL.B', 'CHANNEL.C', 'CHANNEL.R', 'NUM_BO.1', 'NUM_BO.2',\n",
    "       'NUM_BO.3', 'NUM_BO.4', 'FIRST_FLAG.N', 'FIRST_FLAG.Y', 'PURPOSE.C',\n",
    "       'PURPOSE.P', 'PURPOSE.R', 'PROP.CO', 'PROP.CP', 'PROP.MH', 'PROP.PU',\n",
    "       'PROP.SF', 'OCC_STAT.I', 'OCC_STAT.P', 'OCC_STAT.S',\n",
    "       'HIGH_BALANCE_LOAN_INDICATOR.N', 'HIGH_BALANCE_LOAN_INDICATOR.Y',\n",
    "       'ORIG_RATE', 'CURR_RATE', 'ORIG_UPB', 'ORIG_TERM', 'OLTV', 'OCLTV',\n",
    "       'DTI', 'CSCORE_B', 'CSCORE_C', 'MI_PCT', 'NO_UNITS.1', 'NO_UNITS.2',\n",
    "       'NO_UNITS.3', 'NO_UNITS.4', 'MI_TYPE.BPMI', 'MI_TYPE.LPMI',\n",
    "       'MI_TYPE.None', 'max_deliq_6m',\n",
    "       'max_deliq_12m', 'max_deliq_6_12', 'if_deliq', 'deliq_stat_6m',\n",
    "       'deliq_stat_12m', 'deliq_stat_avg_3_12', 'deliq_stat_avg_6_12',\n",
    "       'ACT_PERIOD', 'CURRENT_UPB', 'LOAN_AGE',\n",
    "       'ADJ_REM_MONTHS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4115f5ea-aa2f-4b06-9e80-3f181803d917",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sksurv.linear_model import CoxPHSurvivalAnalysis, CoxnetSurvivalAnalysis\n",
    "\n",
    "def CoxPH(train, test, features, y_train):\n",
    "    clf = CoxnetSurvivalAnalysis(alpha_min_ratio=0.0000000001)\n",
    "    clf.fit(train[features], y_train)\n",
    "    preds = clf.predict(test[features])\n",
    "    return preds\n",
    "\n",
    "def CoxPH_result_wrapper(train, test, features, y_train, y_test, times, verbose):\n",
    "    model_output = CoxPH(\n",
    "        train, test, features, y_train\n",
    "    )\n",
    "    print(model_output)\n",
    "    results = calc_metrics(y_train, y_test, model_output, times)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "91b5e568-2695-45a6-b7b0-8e2374effde3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.23680198 -1.89893868 -2.25677145 ...  1.75710022 -2.10793946\n",
      " -0.99585944]\n",
      "{'cumulative_dynamic_auc': (array([0.95591541, 0.93050849, 0.93098464, 0.92680874, 0.92707979,\n",
      "       0.92661775, 0.92882035, 0.92833246, 0.92877831, 0.92843606,\n",
      "       0.92650988, 0.92725441, 0.92624777, 0.92796277, 0.92615412,\n",
      "       0.92742071, 0.92851101, 0.92845724, 0.9292104 , 0.92699317,\n",
      "       0.92481228, 0.92705387, 0.92871773, 0.93136306, 0.93441351,\n",
      "       0.93679101, 0.93781157, 0.93955903, 0.94068027, 0.94165893,\n",
      "       0.94241289, 0.94455029, 0.94589173, 0.9458088 , 0.94755544,\n",
      "       0.94991099, 0.94989514, 0.9505056 , 0.95228266, 0.95491225,\n",
      "       0.95664737, 0.95465448, 0.95380289, 0.95569853, 0.9557101 ,\n",
      "       0.9555268 , 0.95725701, 0.95699039, 0.95579799, 0.93601016,\n",
      "       0.92709703, 0.92592783, 0.9271408 , 0.92490187, 0.92645675,\n",
      "       0.92639176, 0.9289568 , 0.93008427, 0.92935168, 0.92914896,\n",
      "       0.93126967, 0.92817348, 0.93033302, 0.93343349, 0.93463303,\n",
      "       0.93578978, 0.93860535, 0.94034802, 0.94262716]), 0.9376142390020177), 'concordance_index_ipcw': (0.8676804268695847, 2237291, 255853, 0, 44876)}\n"
     ]
    }
   ],
   "source": [
    "results_CoxPH = CoxPH_result_wrapper(\n",
    "    train,\n",
    "    test,\n",
    "    features,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    times_data,\n",
    "    False\n",
    ")\n",
    "\n",
    "print(results_CoxPH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd501181-5e1e-4cc5-9270-e42df3e6b0ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CoxPH Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d15e45ba-c1be-4301-9696-06da936415d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['CHANNEL.B', 'CHANNEL.C', 'CHANNEL.R', 'NUM_BO.1', 'NUM_BO.2',\n",
    "       'NUM_BO.3', 'NUM_BO.4', 'FIRST_FLAG.N', 'FIRST_FLAG.Y', 'PURPOSE.C',\n",
    "       'PURPOSE.P', 'PURPOSE.R', 'PROP.CO', 'PROP.CP', 'PROP.MH', 'PROP.PU',\n",
    "       'PROP.SF', 'OCC_STAT.I', 'OCC_STAT.P', 'OCC_STAT.S',\n",
    "       'HIGH_BALANCE_LOAN_INDICATOR.N', 'HIGH_BALANCE_LOAN_INDICATOR.Y',\n",
    "       'ORIG_RATE', 'CURR_RATE', 'ORIG_UPB', 'ORIG_TERM', 'OLTV', 'OCLTV',\n",
    "       'DTI', 'CSCORE_B', 'CSCORE_C', 'MI_PCT', 'NO_UNITS.1', 'NO_UNITS.2',\n",
    "       'NO_UNITS.3', 'NO_UNITS.4', 'MI_TYPE.BPMI', 'MI_TYPE.LPMI',\n",
    "       'MI_TYPE.None', 'max_deliq_6m',\n",
    "       'max_deliq_12m', 'max_deliq_6_12', 'if_deliq', 'deliq_stat_6m',\n",
    "       'deliq_stat_12m', 'deliq_stat_avg_3_12', 'deliq_stat_avg_6_12',\n",
    "       'ACT_PERIOD', 'CURRENT_UPB', 'LOAN_AGE',\n",
    "       'ADJ_REM_MONTHS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17333c48-1d49-422f-9479-17f5393c151d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7e9c490-3a01-4110-ba87-8129a3aed39a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sksurv.linear_model import CoxPHSurvivalAnalysis, CoxnetSurvivalAnalysis\n",
    "\n",
    "def CoxPH_elastic_net(train, test, features, y_train):\n",
    "    clf = CoxnetSurvivalAnalysis(l1_ratio=0.9, alpha_min_ratio=0.01)\n",
    "    clf.fit(train[features], y_train)\n",
    "    preds = clf.predict(test[features])\n",
    "    return preds\n",
    "\n",
    "def CoxPH_elastic_net_result_wrapper(train, test, features, y_train, y_test, times, verbose):\n",
    "    model_output = CoxPH_elastic_net(\n",
    "        train, test, features, y_train\n",
    "    )\n",
    "    results = calc_metrics(y_train, y_test, model_output, times)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b84a4235-3520-4049-9999-ec734ff32ac4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cumulative_dynamic_auc': (array([0.79289933, 0.79804003, 0.79498516, 0.78516331, 0.7845892 ,\n",
      "       0.79300953, 0.79410012, 0.80252137, 0.80434738, 0.80846009,\n",
      "       0.80733134, 0.81435846, 0.8193588 , 0.82281734, 0.82386557,\n",
      "       0.82685245, 0.83087044, 0.83272789, 0.83680701, 0.8366192 ,\n",
      "       0.83675338, 0.8409931 , 0.84439392, 0.84743422, 0.85185212,\n",
      "       0.85506367, 0.85613787, 0.8592648 , 0.86092361, 0.86198819,\n",
      "       0.86167847, 0.86200542, 0.85980335, 0.86174869, 0.86370821,\n",
      "       0.86571611, 0.86921171, 0.86975172, 0.87059128, 0.87393784,\n",
      "       0.87336672, 0.87419702, 0.87059094, 0.87220332, 0.8718722 ,\n",
      "       0.86966343, 0.86699187, 0.86745832, 0.86732695, 0.84005605,\n",
      "       0.83269275, 0.83300132, 0.83307961, 0.8314125 , 0.83404954,\n",
      "       0.83488646, 0.83625049, 0.83648748, 0.83128349, 0.83129946,\n",
      "       0.82871515, 0.82769381, 0.82977671, 0.83036305, 0.8280578 ,\n",
      "       0.82397384, 0.82088493, 0.82114766, 0.8218365 ]), 0.8318605526680098), 'concordance_index_ipcw': (0.7377227843080821, 1932619, 560525, 0, 44876)}\n"
     ]
    }
   ],
   "source": [
    "results_CoxPH_elastic_net = CoxPH_elastic_net_result_wrapper(\n",
    "    train,\n",
    "    test,\n",
    "    features,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    times_data,\n",
    "    False\n",
    ")\n",
    "\n",
    "print(results_CoxPH_elastic_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbb90a4-1b1f-4b89-85d2-4d87785d5afc",
   "metadata": {},
   "source": [
    "# Neural Network - DeepHit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c80afa4-52ba-430b-bff0-d26cb7ad337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# For preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper \n",
    "\n",
    "import torch # For building the networks \n",
    "import torchtuples as tt # Some useful functions\n",
    "\n",
    "from pycox.datasets import metabric\n",
    "from pycox.models import DeepHitSingle\n",
    "from pycox.evaluation import EvalSurv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "efaaabca-a43f-4962-875c-69951a8fd49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "_ = torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19ebdc03-741a-41b0-8522-1f132998d527",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_standardize = train[features].columns.tolist()\n",
    "cols_leave = []\n",
    "\n",
    "standardize = [([col], StandardScaler()) for col in cols_standardize]\n",
    "leave = [(col, None) for col in cols_leave]\n",
    "\n",
    "x_mapper = DataFrameMapper(standardize + leave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "304215d5-9d49-48ee-bd70-7711ca279e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_mapper.fit_transform(train).astype('float32')\n",
    "x_test = x_mapper.transform(test).astype('float32')\n",
    "x_val = x_mapper.transform(val).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7aa9d9c9-24ed-479a-8bd2-1a34f3d33c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_durations = 70\n",
    "labtrans = DeepHitSingle.label_transform(num_durations)\n",
    "get_target = lambda df: (df['time_to_event'].values, df['status'].values)\n",
    "y_train = labtrans.fit_transform(*get_target(train))\n",
    "y_val = labtrans.transform(*get_target(val))\n",
    "\n",
    "train_NN = (x_train, y_train)\n",
    "val = (x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "26833a45-3cfc-43e9-9374-1bde21b90a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12613, 51)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a7d1d433-1efa-4b96-9115-f69c484bf2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = x_train.shape[1]\n",
    "num_nodes = [32, 32]\n",
    "out_features = labtrans.out_features\n",
    "batch_norm = True\n",
    "dropout = 0.1\n",
    "\n",
    "net = tt.practical.MLPVanilla(in_features, num_nodes, out_features, batch_norm, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c625ee19-ecc8-4651-877c-3517d20498b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepHitSingle(net, tt.optim.Adam, alpha=0.2, sigma=0.1, duration_index=labtrans.cuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "06626bed-72c9-446c-8e74-678af1a16ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "lr_finder = model.lr_finder(x_train, y_train, batch_size, tolerance=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec5b617d-1606-4c72-a34d-db02ad15ada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.set_lr(lr_finder.get_best_lr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "37b39189-57b7-43f2-af9a-d8f5ee2c92b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[0s / 0s],\t\ttrain_loss: 0.3070,\tval_loss: 0.1758\n",
      "1:\t[0s / 0s],\t\ttrain_loss: 0.1706,\tval_loss: 0.1654\n",
      "2:\t[0s / 1s],\t\ttrain_loss: 0.1639,\tval_loss: 0.1640\n",
      "3:\t[0s / 1s],\t\ttrain_loss: 0.1618,\tval_loss: 0.1617\n",
      "4:\t[0s / 1s],\t\ttrain_loss: 0.1563,\tval_loss: 0.1594\n",
      "5:\t[0s / 2s],\t\ttrain_loss: 0.1519,\tval_loss: 0.1639\n",
      "6:\t[0s / 2s],\t\ttrain_loss: 0.1538,\tval_loss: 0.1586\n",
      "7:\t[0s / 3s],\t\ttrain_loss: 0.1501,\tval_loss: 0.1545\n",
      "8:\t[0s / 3s],\t\ttrain_loss: 0.1475,\tval_loss: 0.1579\n",
      "9:\t[0s / 3s],\t\ttrain_loss: 0.1497,\tval_loss: 0.1537\n",
      "10:\t[0s / 4s],\t\ttrain_loss: 0.1456,\tval_loss: 0.1509\n",
      "11:\t[0s / 4s],\t\ttrain_loss: 0.1457,\tval_loss: 0.1498\n",
      "12:\t[0s / 5s],\t\ttrain_loss: 0.1445,\tval_loss: 0.1497\n",
      "13:\t[0s / 5s],\t\ttrain_loss: 0.1461,\tval_loss: 0.1506\n",
      "14:\t[0s / 6s],\t\ttrain_loss: 0.1399,\tval_loss: 0.1465\n",
      "15:\t[0s / 6s],\t\ttrain_loss: 0.1383,\tval_loss: 0.1473\n",
      "16:\t[0s / 6s],\t\ttrain_loss: 0.1491,\tval_loss: 0.1522\n",
      "17:\t[0s / 7s],\t\ttrain_loss: 0.1382,\tval_loss: 0.1432\n",
      "18:\t[0s / 7s],\t\ttrain_loss: 0.1423,\tval_loss: 0.1460\n",
      "19:\t[0s / 7s],\t\ttrain_loss: 0.1427,\tval_loss: 0.1529\n",
      "20:\t[0s / 8s],\t\ttrain_loss: 0.1545,\tval_loss: 0.1468\n",
      "21:\t[0s / 8s],\t\ttrain_loss: 0.1397,\tval_loss: 0.1423\n",
      "22:\t[0s / 9s],\t\ttrain_loss: 0.1357,\tval_loss: 0.1418\n",
      "23:\t[0s / 9s],\t\ttrain_loss: 0.1381,\tval_loss: 0.1443\n",
      "24:\t[0s / 10s],\t\ttrain_loss: 0.1354,\tval_loss: 0.1442\n",
      "25:\t[0s / 10s],\t\ttrain_loss: 0.1336,\tval_loss: 0.1477\n",
      "26:\t[0s / 10s],\t\ttrain_loss: 0.1352,\tval_loss: 0.1434\n",
      "27:\t[0s / 11s],\t\ttrain_loss: 0.1336,\tval_loss: 0.1465\n",
      "28:\t[0s / 11s],\t\ttrain_loss: 0.1323,\tval_loss: 0.1436\n",
      "29:\t[0s / 11s],\t\ttrain_loss: 0.1337,\tval_loss: 0.1413\n",
      "30:\t[0s / 12s],\t\ttrain_loss: 0.1348,\tval_loss: 0.1456\n",
      "31:\t[0s / 12s],\t\ttrain_loss: 0.1441,\tval_loss: 0.1478\n",
      "32:\t[0s / 13s],\t\ttrain_loss: 0.1349,\tval_loss: 0.1397\n",
      "33:\t[0s / 13s],\t\ttrain_loss: 0.1286,\tval_loss: 0.1399\n",
      "34:\t[0s / 13s],\t\ttrain_loss: 0.1297,\tval_loss: 0.1454\n",
      "35:\t[0s / 14s],\t\ttrain_loss: 0.1374,\tval_loss: 0.1474\n",
      "36:\t[0s / 14s],\t\ttrain_loss: 0.1330,\tval_loss: 0.1376\n",
      "37:\t[0s / 15s],\t\ttrain_loss: 0.1276,\tval_loss: 0.1444\n",
      "38:\t[0s / 15s],\t\ttrain_loss: 0.1263,\tval_loss: 0.1451\n",
      "39:\t[0s / 15s],\t\ttrain_loss: 0.1275,\tval_loss: 0.1384\n",
      "40:\t[0s / 16s],\t\ttrain_loss: 0.1256,\tval_loss: 0.1370\n",
      "41:\t[0s / 16s],\t\ttrain_loss: 0.1293,\tval_loss: 0.1371\n",
      "42:\t[0s / 16s],\t\ttrain_loss: 0.1254,\tval_loss: 0.1343\n",
      "43:\t[0s / 17s],\t\ttrain_loss: 0.1239,\tval_loss: 0.1358\n",
      "44:\t[0s / 17s],\t\ttrain_loss: 0.1250,\tval_loss: 0.1378\n",
      "45:\t[0s / 18s],\t\ttrain_loss: 0.1257,\tval_loss: 0.1405\n",
      "46:\t[0s / 18s],\t\ttrain_loss: 0.1230,\tval_loss: 0.1389\n",
      "47:\t[0s / 18s],\t\ttrain_loss: 0.1201,\tval_loss: 0.1397\n",
      "48:\t[0s / 19s],\t\ttrain_loss: 0.1260,\tval_loss: 0.1382\n",
      "49:\t[0s / 19s],\t\ttrain_loss: 0.1238,\tval_loss: 0.1433\n",
      "50:\t[0s / 20s],\t\ttrain_loss: 0.1227,\tval_loss: 0.1508\n",
      "51:\t[0s / 20s],\t\ttrain_loss: 0.1306,\tval_loss: 0.1354\n",
      "52:\t[0s / 20s],\t\ttrain_loss: 0.1229,\tval_loss: 0.1386\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "callbacks = [tt.callbacks.EarlyStopping()]\n",
    "log = model.fit(x_train, y_train, batch_size, epochs, callbacks, val_data=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "df461c87-797b-4878-9628-07951a2aea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "surv = model.interpolate(10).predict_surv_df(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6ed8da49-386f-4c57-a9f6-9fa29cae087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "durations_test, events_test = get_target(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9387e4e8-5ea5-44a6-858e-60be5a2887cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9586742683134227"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ev = EvalSurv(surv, durations_test, events_test, censor_surv='km')\n",
    "ev.concordance_td('antolini')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482b3ada-dc43-4a54-a1a9-9e7915297f2a",
   "metadata": {},
   "source": [
    "# Neural Network - DeepSurv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0bf041f3-8926-44f4-9716-eb34975ac073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# For preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper \n",
    "\n",
    "import torch # For building the networks \n",
    "import torchtuples as tt # Some useful functions\n",
    "\n",
    "from pycox.datasets import metabric\n",
    "from pycox.models import CoxCC\n",
    "from pycox.evaluation import EvalSurv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf63aa8f-9c6d-4db5-b70d-59d3f814c647",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "_ = torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2332a38e-a0e4-4441-adf9-67d213cb5aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = x_train.shape[1]\n",
    "num_nodes = [32, 32]\n",
    "out_features = 1\n",
    "batch_norm = True\n",
    "dropout = 0.1\n",
    "output_bias = False\n",
    "\n",
    "net = tt.practical.MLPVanilla(in_features, num_nodes, out_features, batch_norm,\n",
    "                              dropout, output_bias=output_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57105d6e-3a6a-4dd0-804a-3766fd2edada",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CoxCC(net, tt.optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af468748-d56b-42d4-a400-d5266edd1e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 256\n",
    "lrfinder = model.lr_finder(x_train, y_train, batch_size, tolerance=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "737ac749-6af2-4fee-8dc6-7d3e29afc787",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.set_lr(lr_finder.get_best_lr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9161eff5-e18c-4806-968d-8c4b04ff0d43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[0s / 0s],\t\ttrain_loss: 0.4185,\tval_loss: 0.4784\n",
      "1:\t[0s / 0s],\t\ttrain_loss: 0.2910,\tval_loss: 0.3235\n",
      "2:\t[0s / 0s],\t\ttrain_loss: 0.2568,\tval_loss: 0.3707\n",
      "3:\t[0s / 0s],\t\ttrain_loss: 0.2796,\tval_loss: 0.3080\n",
      "4:\t[0s / 0s],\t\ttrain_loss: 0.2367,\tval_loss: 0.3551\n",
      "5:\t[0s / 0s],\t\ttrain_loss: 0.2382,\tval_loss: 0.3246\n",
      "6:\t[0s / 0s],\t\ttrain_loss: 0.2383,\tval_loss: 0.3050\n",
      "7:\t[0s / 0s],\t\ttrain_loss: 0.2239,\tval_loss: 0.2758\n",
      "8:\t[0s / 0s],\t\ttrain_loss: 0.2144,\tval_loss: 0.2528\n",
      "9:\t[0s / 0s],\t\ttrain_loss: 0.2275,\tval_loss: 0.2270\n",
      "10:\t[0s / 0s],\t\ttrain_loss: 0.2173,\tval_loss: 0.3012\n",
      "11:\t[0s / 0s],\t\ttrain_loss: 0.2025,\tval_loss: 0.2415\n",
      "12:\t[0s / 0s],\t\ttrain_loss: 0.1986,\tval_loss: 0.2597\n",
      "13:\t[0s / 0s],\t\ttrain_loss: 0.2028,\tval_loss: 0.3292\n",
      "14:\t[0s / 0s],\t\ttrain_loss: 0.2092,\tval_loss: 0.3108\n",
      "15:\t[0s / 0s],\t\ttrain_loss: 0.2153,\tval_loss: 0.2118\n",
      "16:\t[0s / 0s],\t\ttrain_loss: 0.2106,\tval_loss: 0.2498\n",
      "17:\t[0s / 0s],\t\ttrain_loss: 0.2023,\tval_loss: 0.2895\n",
      "18:\t[0s / 0s],\t\ttrain_loss: 0.1871,\tval_loss: 0.2389\n",
      "19:\t[0s / 0s],\t\ttrain_loss: 0.1835,\tval_loss: 0.2299\n",
      "20:\t[0s / 0s],\t\ttrain_loss: 0.1707,\tval_loss: 0.1989\n",
      "21:\t[0s / 0s],\t\ttrain_loss: 0.1953,\tval_loss: 0.2454\n",
      "22:\t[0s / 0s],\t\ttrain_loss: 0.1759,\tval_loss: 0.2429\n",
      "23:\t[0s / 0s],\t\ttrain_loss: 0.2022,\tval_loss: 0.1626\n",
      "24:\t[0s / 0s],\t\ttrain_loss: 0.1600,\tval_loss: 0.2920\n",
      "25:\t[0s / 0s],\t\ttrain_loss: 0.1751,\tval_loss: 0.2443\n",
      "26:\t[0s / 0s],\t\ttrain_loss: 0.1879,\tval_loss: 0.2083\n",
      "27:\t[0s / 0s],\t\ttrain_loss: 0.1742,\tval_loss: 0.2483\n",
      "28:\t[0s / 0s],\t\ttrain_loss: 0.1728,\tval_loss: 0.2868\n",
      "29:\t[0s / 0s],\t\ttrain_loss: 0.1762,\tval_loss: 0.2021\n",
      "30:\t[0s / 0s],\t\ttrain_loss: 0.1606,\tval_loss: 0.2245\n",
      "31:\t[0s / 0s],\t\ttrain_loss: 0.1486,\tval_loss: 0.1803\n",
      "32:\t[0s / 1s],\t\ttrain_loss: 0.1534,\tval_loss: 0.2912\n",
      "33:\t[0s / 1s],\t\ttrain_loss: 0.1710,\tval_loss: 0.2300\n"
     ]
    }
   ],
   "source": [
    "epochs = 256\n",
    "callbacks = [tt.callbacks.EarlyStopping()]\n",
    "verbose = True\n",
    "\n",
    "log = model.fit(x_train, y_train, batch_size, epochs, callbacks, verbose,\n",
    "                val_data=val, val_batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0011347d-cd5a-410e-9b83-340d4a441d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.compute_baseline_hazards()\n",
    "\n",
    "surv = model.predict_surv_df(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e0f6ac8-7b28-4a85-a0bd-9b7f0f1d5d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "durations_test, events_test = get_target(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3776b07c-4179-4160-88bf-4602fd30ce6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9247491378316661"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ev = EvalSurv(surv, durations_test, events_test, censor_surv='km')\n",
    "ev.concordance_td()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34700f1-1553-4293-882c-89f791525f95",
   "metadata": {},
   "source": [
    "# NGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d2ebf0df-8524-459b-9f57-2a0cf25a8bc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ngboost import NGBSurvival\n",
    "from ngboost.distns import LogNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "85f6178d-087b-4bf0-a691-50ad57f89706",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['CHANNEL.B', 'CHANNEL.C', 'CHANNEL.R', 'NUM_BO.1', 'NUM_BO.2',\n",
    "       'NUM_BO.3', 'NUM_BO.4', 'FIRST_FLAG.N', 'FIRST_FLAG.Y', 'PURPOSE.C',\n",
    "       'PURPOSE.P', 'PURPOSE.R', 'PROP.CO', 'PROP.CP', 'PROP.MH', 'PROP.PU',\n",
    "       'PROP.SF', 'OCC_STAT.I', 'OCC_STAT.P', 'OCC_STAT.S',\n",
    "       'HIGH_BALANCE_LOAN_INDICATOR.N', 'HIGH_BALANCE_LOAN_INDICATOR.Y',\n",
    "       'ORIG_RATE', 'CURR_RATE', 'ORIG_UPB', 'ORIG_TERM', 'OLTV', 'OCLTV',\n",
    "       'DTI', 'CSCORE_B', 'CSCORE_C', 'MI_PCT', 'NO_UNITS.1', 'NO_UNITS.2',\n",
    "       'NO_UNITS.3', 'NO_UNITS.4', 'MI_TYPE.BPMI', 'MI_TYPE.LPMI',\n",
    "       'MI_TYPE.None', 'max_deliq_6m',\n",
    "       'max_deliq_12m', 'max_deliq_6_12', 'if_deliq', 'deliq_stat_6m',\n",
    "       'deliq_stat_12m', 'deliq_stat_avg_3_12', 'deliq_stat_avg_6_12',\n",
    "       'ACT_PERIOD', 'CURRENT_UPB', 'LOAN_AGE',\n",
    "       'ADJ_REM_MONTHS']\n",
    "target = ['time_to_event']\n",
    "censoring = [\"status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ccf4e8f0-c1e4-4714-9c99-0621b3fd1b1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with combination nr. 0\n",
      "[iter 0] loss=1.4091 val_loss=0.0000 scale=2.0000 norm=2.2946\n",
      "[iter 100] loss=0.6578 val_loss=0.0000 scale=2.0000 norm=0.9093\n",
      "[iter 0] loss=1.4282 val_loss=0.0000 scale=2.0000 norm=2.3314\n",
      "[iter 100] loss=0.6349 val_loss=0.0000 scale=2.0000 norm=0.9164\n",
      "[iter 0] loss=1.4357 val_loss=0.0000 scale=2.0000 norm=2.3345\n",
      "[iter 100] loss=0.6635 val_loss=0.0000 scale=2.0000 norm=0.9216\n",
      "done with combination nr. 1\n",
      "[iter 0] loss=1.4177 val_loss=0.0000 scale=4.0000 norm=4.6471\n",
      "[iter 0] loss=1.4233 val_loss=0.0000 scale=8.0000 norm=9.3027\n",
      "[iter 0] loss=1.4265 val_loss=0.0000 scale=4.0000 norm=4.6485\n",
      "done with combination nr. 2\n",
      "[iter 0] loss=1.4114 val_loss=0.0000 scale=8.0000 norm=9.1934\n",
      "[iter 100] loss=0.5840 val_loss=0.0000 scale=2.0000 norm=0.6750\n",
      "[iter 200] loss=0.5965 val_loss=0.0000 scale=2.0000 norm=0.6298\n",
      "[iter 0] loss=1.4243 val_loss=0.0000 scale=4.0000 norm=4.6637\n",
      "[iter 100] loss=0.6124 val_loss=0.0000 scale=2.0000 norm=0.7125\n",
      "[iter 200] loss=0.6085 val_loss=0.0000 scale=2.0000 norm=0.6629\n",
      "[iter 0] loss=1.4444 val_loss=0.0000 scale=8.0000 norm=9.3523\n",
      "[iter 100] loss=0.6171 val_loss=0.0000 scale=2.0000 norm=0.7002\n",
      "[iter 200] loss=0.5890 val_loss=0.0000 scale=2.0000 norm=0.6338\n",
      "done with combination nr. 3\n",
      "[iter 0] loss=1.4382 val_loss=0.0000 scale=4.0000 norm=4.6172\n",
      "[iter 0] loss=1.4112 val_loss=0.0000 scale=8.0000 norm=9.2594\n",
      "[iter 0] loss=1.4180 val_loss=0.0000 scale=4.0000 norm=4.6996\n",
      "done with combination nr. 4\n",
      "[iter 0] loss=1.4087 val_loss=0.0000 scale=4.0000 norm=4.6089\n",
      "[iter 100] loss=2.2440 val_loss=0.0000 scale=2.0000 norm=2.9266\n",
      "[iter 0] loss=1.4283 val_loss=0.0000 scale=4.0000 norm=4.6450\n",
      "[iter 100] loss=nan val_loss=0.0000 scale=0.0001 norm=0.0002\n",
      "[iter 0] loss=1.4261 val_loss=0.0000 scale=4.0000 norm=4.6704\n",
      "[iter 100] loss=nan val_loss=0.0000 scale=0.0001 norm=0.0002\n",
      "done with combination nr. 5\n",
      "[iter 0] loss=1.4149 val_loss=0.0000 scale=4.0000 norm=4.6158\n",
      "[iter 100] loss=0.7626 val_loss=0.0000 scale=0.0002 norm=0.0004\n",
      "[iter 0] loss=1.4206 val_loss=0.0000 scale=4.0000 norm=4.6652\n",
      "[iter 100] loss=0.8353 val_loss=0.0000 scale=0.0001 norm=0.0003\n",
      "[iter 0] loss=1.4319 val_loss=0.0000 scale=4.0000 norm=4.6655\n",
      "[iter 100] loss=0.8089 val_loss=0.0000 scale=0.0002 norm=0.0005\n",
      "done with combination nr. 6\n",
      "[iter 0] loss=1.4229 val_loss=0.0000 scale=4.0000 norm=4.6273\n",
      "[iter 100] loss=0.5668 val_loss=0.0000 scale=2.0000 norm=0.5872\n",
      "[iter 200] loss=0.5283 val_loss=0.0000 scale=1.0000 norm=0.2541\n",
      "[iter 0] loss=1.4180 val_loss=0.0000 scale=4.0000 norm=4.6598\n",
      "[iter 100] loss=0.5529 val_loss=0.0000 scale=2.0000 norm=0.5679\n",
      "[iter 200] loss=0.5198 val_loss=0.0000 scale=1.0000 norm=0.2503\n",
      "[iter 0] loss=1.4266 val_loss=0.0000 scale=4.0000 norm=4.6598\n",
      "[iter 100] loss=0.5667 val_loss=0.0000 scale=2.0000 norm=0.6052\n",
      "[iter 200] loss=0.5268 val_loss=0.0000 scale=1.0000 norm=0.2588\n",
      "done with combination nr. 7\n",
      "[iter 0] loss=1.4114 val_loss=0.0000 scale=8.0000 norm=9.1956\n",
      "[iter 100] loss=0.6516 val_loss=0.0000 scale=4.0000 norm=1.7061\n",
      "[iter 0] loss=1.4230 val_loss=0.0000 scale=4.0000 norm=4.6618\n",
      "[iter 100] loss=0.6555 val_loss=0.0000 scale=4.0000 norm=1.7401\n",
      "[iter 0] loss=1.4329 val_loss=0.0000 scale=4.0000 norm=4.6865\n",
      "[iter 100] loss=0.6763 val_loss=0.0000 scale=4.0000 norm=1.8061\n",
      "done with combination nr. 8\n",
      "[iter 0] loss=1.4108 val_loss=0.0000 scale=8.0000 norm=9.1298\n",
      "[iter 100] loss=nan val_loss=0.0000 scale=0.0000 norm=0.0003\n",
      "[iter 200] loss=nan val_loss=0.0000 scale=0.0001 norm=0.0006\n",
      "[iter 300] loss=nan val_loss=0.0000 scale=0.0001 norm=0.0006\n",
      "[iter 0] loss=1.4267 val_loss=0.0000 scale=4.0000 norm=4.6197\n",
      "[iter 100] loss=0.9706 val_loss=0.0000 scale=2.0000 norm=5.1116\n",
      "[iter 200] loss=0.9890 val_loss=0.0000 scale=2.0000 norm=10.7551\n",
      "[iter 300] loss=1.3379 val_loss=0.0000 scale=2.0000 norm=19.7076\n",
      "[iter 0] loss=1.4092 val_loss=0.0000 scale=8.0000 norm=9.4541\n",
      "[iter 100] loss=nan val_loss=0.0000 scale=0.0000 norm=0.0004\n",
      "[iter 200] loss=nan val_loss=0.0000 scale=0.0000 norm=0.0004\n",
      "[iter 300] loss=nan val_loss=0.0000 scale=0.0000 norm=0.0004\n",
      "done with combination nr. 9\n",
      "[iter 0] loss=1.4255 val_loss=0.0000 scale=4.0000 norm=4.6261\n",
      "[iter 100] loss=0.3165 val_loss=0.0000 scale=2.0000 norm=0.2245\n",
      "[iter 200] loss=0.2219 val_loss=0.0000 scale=1.0000 norm=0.0799\n",
      "[iter 0] loss=1.4100 val_loss=0.0000 scale=4.0000 norm=4.6449\n",
      "[iter 100] loss=0.2676 val_loss=0.0000 scale=1.0000 norm=0.0892\n",
      "[iter 200] loss=0.1924 val_loss=0.0000 scale=1.0000 norm=0.0656\n",
      "[iter 0] loss=1.4319 val_loss=0.0000 scale=4.0000 norm=4.6757\n",
      "[iter 100] loss=0.3185 val_loss=0.0000 scale=2.0000 norm=0.2304\n",
      "[iter 200] loss=0.2590 val_loss=0.0000 scale=1.0000 norm=0.1014\n",
      "done with combination nr. 10\n",
      "[iter 0] loss=1.4085 val_loss=0.0000 scale=2.0000 norm=2.3374\n",
      "[iter 0] loss=1.4464 val_loss=0.0000 scale=4.0000 norm=4.6668\n",
      "[iter 0] loss=1.4125 val_loss=0.0000 scale=2.0000 norm=2.3022\n",
      "done with combination nr. 11\n",
      "[iter 0] loss=1.4256 val_loss=0.0000 scale=2.0000 norm=2.3307\n",
      "[iter 100] loss=0.1793 val_loss=0.0000 scale=2.0000 norm=0.1205\n",
      "[iter 200] loss=0.0308 val_loss=0.0000 scale=1.0000 norm=0.0401\n",
      "[iter 300] loss=-0.0790 val_loss=0.0000 scale=1.0000 norm=0.0398\n",
      "[iter 0] loss=1.4281 val_loss=0.0000 scale=2.0000 norm=2.3110\n",
      "[iter 100] loss=0.1901 val_loss=0.0000 scale=2.0000 norm=0.1266\n",
      "[iter 200] loss=0.0401 val_loss=0.0000 scale=1.0000 norm=0.0422\n",
      "[iter 300] loss=-0.0674 val_loss=0.0000 scale=1.0000 norm=0.0377\n",
      "[iter 0] loss=1.4177 val_loss=0.0000 scale=2.0000 norm=2.3161\n",
      "[iter 100] loss=0.1667 val_loss=0.0000 scale=1.0000 norm=0.0562\n",
      "[iter 200] loss=0.0232 val_loss=0.0000 scale=1.0000 norm=0.0392\n",
      "[iter 300] loss=-0.0749 val_loss=0.0000 scale=0.5000 norm=0.0180\n",
      "done with combination nr. 12\n",
      "[iter 0] loss=1.4185 val_loss=0.0000 scale=4.0000 norm=4.6436\n",
      "[iter 0] loss=1.4279 val_loss=0.0000 scale=4.0000 norm=4.6445\n",
      "[iter 0] loss=1.4211 val_loss=0.0000 scale=4.0000 norm=4.6589\n",
      "done with combination nr. 13\n",
      "[iter 0] loss=1.4156 val_loss=0.0000 scale=8.0000 norm=9.2612\n",
      "[iter 100] loss=0.5536 val_loss=0.0000 scale=2.0000 norm=0.5727\n",
      "[iter 200] loss=0.5158 val_loss=0.0000 scale=1.0000 norm=0.2478\n",
      "[iter 0] loss=1.4138 val_loss=0.0000 scale=4.0000 norm=4.6284\n",
      "[iter 100] loss=0.5524 val_loss=0.0000 scale=2.0000 norm=0.5846\n",
      "[iter 200] loss=0.5188 val_loss=0.0000 scale=1.0000 norm=0.2569\n",
      "[iter 0] loss=1.4379 val_loss=0.0000 scale=4.0000 norm=4.6869\n",
      "[iter 100] loss=0.5801 val_loss=0.0000 scale=2.0000 norm=0.6013\n",
      "[iter 200] loss=0.5371 val_loss=0.0000 scale=2.0000 norm=0.5165\n",
      "done with combination nr. 14\n",
      "[iter 0] loss=1.4375 val_loss=0.0000 scale=8.0000 norm=9.4024\n",
      "[iter 100] loss=0.5952 val_loss=0.0000 scale=2.0000 norm=0.6887\n",
      "[iter 200] loss=0.5925 val_loss=0.0000 scale=2.0000 norm=0.6294\n",
      "[iter 0] loss=1.4434 val_loss=0.0000 scale=8.0000 norm=9.2430\n",
      "[iter 100] loss=0.6126 val_loss=0.0000 scale=2.0000 norm=0.7136\n",
      "[iter 200] loss=0.6102 val_loss=0.0000 scale=2.0000 norm=0.6389\n",
      "[iter 0] loss=1.4169 val_loss=0.0000 scale=8.0000 norm=9.3337\n",
      "[iter 100] loss=0.5863 val_loss=0.0000 scale=2.0000 norm=0.6928\n",
      "[iter 200] loss=0.6002 val_loss=0.0000 scale=2.0000 norm=0.6187\n",
      "done with combination nr. 15\n",
      "[iter 0] loss=1.4323 val_loss=0.0000 scale=2.0000 norm=2.3304\n",
      "[iter 100] loss=0.0215 val_loss=0.0000 scale=0.5000 norm=0.0328\n",
      "[iter 0] loss=1.4081 val_loss=0.0000 scale=4.0000 norm=4.6176\n",
      "[iter 100] loss=nan val_loss=0.0000 scale=0.0002 norm=0.0002\n",
      "[iter 0] loss=1.4270 val_loss=0.0000 scale=2.0000 norm=2.3340\n",
      "[iter 100] loss=0.0444 val_loss=0.0000 scale=2.0000 norm=0.1174\n",
      "done with combination nr. 16\n",
      "[iter 0] loss=1.4263 val_loss=0.0000 scale=2.0000 norm=2.3098\n",
      "[iter 100] loss=0.6435 val_loss=0.0000 scale=2.0000 norm=0.9288\n",
      "[iter 200] loss=0.4749 val_loss=0.0000 scale=2.0000 norm=0.4963\n",
      "[iter 0] loss=1.4097 val_loss=0.0000 scale=2.0000 norm=2.3131\n",
      "[iter 100] loss=0.6482 val_loss=0.0000 scale=2.0000 norm=0.9259\n",
      "[iter 200] loss=0.4605 val_loss=0.0000 scale=2.0000 norm=0.4959\n",
      "[iter 0] loss=1.3833 val_loss=0.0000 scale=2.0000 norm=2.2993\n",
      "[iter 100] loss=0.6447 val_loss=0.0000 scale=2.0000 norm=0.9008\n",
      "[iter 200] loss=0.4657 val_loss=0.0000 scale=2.0000 norm=0.4791\n",
      "done with combination nr. 17\n",
      "[iter 0] loss=1.4522 val_loss=0.0000 scale=4.0000 norm=4.7519\n",
      "[iter 0] loss=1.4181 val_loss=0.0000 scale=4.0000 norm=4.6182\n",
      "[iter 0] loss=1.3887 val_loss=0.0000 scale=8.0000 norm=9.2275\n",
      "done with combination nr. 18\n",
      "[iter 0] loss=1.4154 val_loss=0.0000 scale=8.0000 norm=9.2355\n",
      "[iter 100] loss=0.5774 val_loss=0.0000 scale=1.0000 norm=0.3115\n",
      "[iter 0] loss=1.4247 val_loss=0.0000 scale=8.0000 norm=9.3638\n",
      "[iter 100] loss=0.5840 val_loss=0.0000 scale=2.0000 norm=0.6381\n",
      "[iter 0] loss=1.4272 val_loss=0.0000 scale=8.0000 norm=9.2934\n",
      "[iter 100] loss=0.5911 val_loss=0.0000 scale=2.0000 norm=0.6292\n",
      "done with combination nr. 19\n",
      "[iter 0] loss=1.4152 val_loss=0.0000 scale=4.0000 norm=4.6042\n",
      "[iter 0] loss=1.4269 val_loss=0.0000 scale=2.0000 norm=2.3503\n",
      "[iter 0] loss=1.4207 val_loss=0.0000 scale=2.0000 norm=2.3666\n",
      "done with combination nr. 20\n",
      "[iter 0] loss=1.4447 val_loss=0.0000 scale=4.0000 norm=4.6442\n",
      "[iter 100] loss=0.6634 val_loss=0.0000 scale=2.0000 norm=0.8852\n",
      "[iter 200] loss=0.6076 val_loss=0.0000 scale=2.0000 norm=0.6910\n",
      "[iter 300] loss=0.5557 val_loss=0.0000 scale=2.0000 norm=0.5892\n",
      "[iter 0] loss=1.4506 val_loss=0.0000 scale=4.0000 norm=4.7787\n",
      "[iter 100] loss=0.6413 val_loss=0.0000 scale=2.0000 norm=0.8630\n",
      "[iter 200] loss=0.5715 val_loss=0.0000 scale=2.0000 norm=0.6620\n",
      "[iter 300] loss=0.5458 val_loss=0.0000 scale=2.0000 norm=0.5723\n",
      "[iter 0] loss=1.4079 val_loss=0.0000 scale=8.0000 norm=9.3416\n",
      "[iter 100] loss=0.6549 val_loss=0.0000 scale=2.0000 norm=0.8455\n",
      "[iter 200] loss=0.5455 val_loss=0.0000 scale=2.0000 norm=0.6490\n",
      "[iter 300] loss=0.5247 val_loss=0.0000 scale=2.0000 norm=0.5496\n",
      "done with combination nr. 21\n",
      "[iter 0] loss=1.4274 val_loss=0.0000 scale=8.0000 norm=9.3764\n",
      "[iter 100] loss=0.5225 val_loss=0.0000 scale=2.0000 norm=0.5129\n",
      "[iter 200] loss=0.4679 val_loss=0.0000 scale=1.0000 norm=0.2090\n",
      "[iter 300] loss=0.4373 val_loss=0.0000 scale=1.0000 norm=0.1905\n",
      "[iter 0] loss=1.4205 val_loss=0.0000 scale=4.0000 norm=4.6447\n",
      "[iter 100] loss=0.4954 val_loss=0.0000 scale=2.0000 norm=0.4667\n",
      "[iter 200] loss=0.4522 val_loss=0.0000 scale=1.0000 norm=0.1991\n",
      "[iter 300] loss=0.4283 val_loss=0.0000 scale=1.0000 norm=0.1829\n",
      "[iter 0] loss=1.4195 val_loss=0.0000 scale=4.0000 norm=4.6138\n",
      "[iter 100] loss=0.5096 val_loss=0.0000 scale=2.0000 norm=0.4783\n",
      "[iter 200] loss=0.4618 val_loss=0.0000 scale=1.0000 norm=0.2033\n",
      "[iter 300] loss=0.4397 val_loss=0.0000 scale=0.0625 norm=0.0121\n",
      "done with combination nr. 22\n",
      "[iter 0] loss=1.4132 val_loss=0.0000 scale=2.0000 norm=2.2971\n",
      "[iter 0] loss=1.4220 val_loss=0.0000 scale=2.0000 norm=2.3421\n",
      "[iter 0] loss=1.4321 val_loss=0.0000 scale=2.0000 norm=2.3338\n",
      "done with combination nr. 23\n",
      "[iter 0] loss=1.4256 val_loss=0.0000 scale=8.0000 norm=9.3057\n",
      "[iter 0] loss=1.4236 val_loss=0.0000 scale=8.0000 norm=9.2874\n",
      "[iter 0] loss=1.4183 val_loss=0.0000 scale=8.0000 norm=9.2999\n",
      "done with combination nr. 24\n",
      "[iter 0] loss=1.4246 val_loss=0.0000 scale=8.0000 norm=9.1875\n",
      "[iter 100] loss=0.5992 val_loss=0.0000 scale=2.0000 norm=0.6665\n",
      "[iter 200] loss=0.5884 val_loss=0.0000 scale=0.0020 norm=0.0006\n",
      "[iter 300] loss=0.5804 val_loss=0.0000 scale=2.0000 norm=0.6133\n",
      "[iter 0] loss=1.4114 val_loss=0.0000 scale=8.0000 norm=9.2549\n",
      "[iter 100] loss=0.5704 val_loss=0.0000 scale=2.0000 norm=0.6123\n",
      "[iter 200] loss=0.5417 val_loss=0.0000 scale=2.0000 norm=0.5476\n",
      "[iter 300] loss=0.5274 val_loss=0.0000 scale=0.5000 norm=0.1302\n",
      "[iter 0] loss=1.4311 val_loss=0.0000 scale=4.0000 norm=4.7239\n",
      "[iter 100] loss=0.5866 val_loss=0.0000 scale=2.0000 norm=0.6245\n",
      "[iter 200] loss=0.5683 val_loss=0.0000 scale=0.0039 norm=0.0011\n",
      "[iter 300] loss=0.5633 val_loss=0.0000 scale=0.5000 norm=0.1420\n",
      "done with combination nr. 25\n",
      "[iter 0] loss=1.4024 val_loss=0.0000 scale=4.0000 norm=4.6280\n",
      "[iter 100] loss=0.6300 val_loss=0.0000 scale=2.0000 norm=0.8450\n",
      "[iter 200] loss=0.5896 val_loss=0.0000 scale=4.0000 norm=1.4296\n",
      "[iter 0] loss=1.4209 val_loss=0.0000 scale=4.0000 norm=4.6680\n",
      "[iter 100] loss=0.6544 val_loss=0.0000 scale=2.0000 norm=0.8782\n",
      "[iter 200] loss=0.6072 val_loss=0.0000 scale=2.0000 norm=0.7212\n",
      "[iter 0] loss=1.4260 val_loss=0.0000 scale=8.0000 norm=9.2451\n",
      "[iter 100] loss=0.6780 val_loss=0.0000 scale=2.0000 norm=0.8942\n",
      "[iter 200] loss=0.6106 val_loss=0.0000 scale=2.0000 norm=0.7439\n",
      "done with combination nr. 26\n",
      "[iter 0] loss=1.4255 val_loss=0.0000 scale=4.0000 norm=4.6409\n",
      "[iter 0] loss=1.4278 val_loss=0.0000 scale=4.0000 norm=4.6523\n",
      "[iter 0] loss=1.4143 val_loss=0.0000 scale=4.0000 norm=4.6536\n",
      "done with combination nr. 27\n",
      "[iter 0] loss=1.4417 val_loss=0.0000 scale=8.0000 norm=9.3967\n",
      "[iter 0] loss=1.4179 val_loss=0.0000 scale=8.0000 norm=9.2640\n",
      "[iter 0] loss=1.4024 val_loss=0.0000 scale=8.0000 norm=9.2629\n",
      "done with combination nr. 28\n",
      "[iter 0] loss=1.4073 val_loss=0.0000 scale=4.0000 norm=4.6268\n",
      "[iter 100] loss=0.6322 val_loss=0.0000 scale=2.0000 norm=0.8452\n",
      "[iter 200] loss=0.5691 val_loss=0.0000 scale=2.0000 norm=0.6674\n",
      "[iter 0] loss=1.4234 val_loss=0.0000 scale=4.0000 norm=4.6692\n",
      "[iter 100] loss=0.6442 val_loss=0.0000 scale=4.0000 norm=1.6665\n",
      "[iter 200] loss=0.5819 val_loss=0.0000 scale=2.0000 norm=0.6559\n",
      "[iter 0] loss=1.4366 val_loss=0.0000 scale=4.0000 norm=4.6498\n",
      "[iter 100] loss=0.6654 val_loss=0.0000 scale=2.0000 norm=0.8540\n",
      "[iter 200] loss=0.6011 val_loss=0.0000 scale=2.0000 norm=0.6745\n",
      "done with combination nr. 29\n",
      "[iter 0] loss=1.4285 val_loss=0.0000 scale=4.0000 norm=4.7040\n",
      "[iter 0] loss=1.4467 val_loss=0.0000 scale=4.0000 norm=4.6779\n",
      "[iter 0] loss=1.4101 val_loss=0.0000 scale=4.0000 norm=4.5834\n",
      "done with combination nr. 30\n",
      "[iter 0] loss=1.4400 val_loss=0.0000 scale=4.0000 norm=4.6035\n",
      "[iter 100] loss=0.4150 val_loss=0.0000 scale=1.0000 norm=0.1616\n",
      "[iter 0] loss=1.4191 val_loss=0.0000 scale=2.0000 norm=2.3100\n",
      "[iter 100] loss=0.4082 val_loss=0.0000 scale=1.0000 norm=0.1665\n",
      "[iter 0] loss=1.3902 val_loss=0.0000 scale=4.0000 norm=4.6448\n",
      "[iter 100] loss=0.3807 val_loss=0.0000 scale=2.0000 norm=0.3066\n",
      "done with combination nr. 31\n",
      "[iter 0] loss=1.4057 val_loss=0.0000 scale=4.0000 norm=4.6129\n",
      "[iter 100] loss=0.9581 val_loss=0.0000 scale=1.0000 norm=4.1126\n",
      "[iter 200] loss=nan val_loss=0.0000 scale=0.0001 norm=0.0003\n",
      "[iter 300] loss=nan val_loss=0.0000 scale=0.0001 norm=0.0003\n",
      "[iter 0] loss=1.4391 val_loss=0.0000 scale=4.0000 norm=4.6413\n",
      "[iter 100] loss=1.3949 val_loss=0.0000 scale=2.0000 norm=4.2984\n",
      "[iter 200] loss=1.7823 val_loss=0.0000 scale=2.0000 norm=9.2754\n",
      "[iter 300] loss=nan val_loss=0.0000 scale=0.0001 norm=0.0004\n",
      "[iter 0] loss=1.4370 val_loss=0.0000 scale=4.0000 norm=4.6925\n",
      "[iter 100] loss=nan val_loss=0.0000 scale=0.0001 norm=0.0003\n",
      "[iter 200] loss=nan val_loss=0.0000 scale=0.0001 norm=0.0003\n",
      "[iter 300] loss=nan val_loss=0.0000 scale=0.0001 norm=0.0003\n",
      "done with combination nr. 32\n",
      "[iter 0] loss=1.4211 val_loss=0.0000 scale=4.0000 norm=4.6217\n",
      "[iter 100] loss=0.0977 val_loss=0.0000 scale=1.0000 norm=0.0517\n",
      "[iter 200] loss=-0.0168 val_loss=0.0000 scale=1.0000 norm=0.0404\n",
      "[iter 300] loss=-0.1185 val_loss=0.0000 scale=2.0000 norm=0.0691\n",
      "[iter 0] loss=1.4131 val_loss=0.0000 scale=2.0000 norm=2.3396\n",
      "[iter 100] loss=0.0757 val_loss=0.0000 scale=1.0000 norm=0.0493\n",
      "[iter 200] loss=-0.0770 val_loss=0.0000 scale=1.0000 norm=0.0375\n",
      "[iter 300] loss=-0.1578 val_loss=0.0000 scale=0.2500 norm=0.0089\n",
      "[iter 0] loss=1.4332 val_loss=0.0000 scale=2.0000 norm=2.3229\n",
      "[iter 100] loss=0.1209 val_loss=0.0000 scale=1.0000 norm=0.0528\n",
      "[iter 200] loss=-0.0132 val_loss=0.0000 scale=1.0000 norm=0.0398\n",
      "[iter 300] loss=-0.1071 val_loss=0.0000 scale=1.0000 norm=0.0368\n",
      "done with combination nr. 33\n",
      "[iter 0] loss=1.4180 val_loss=0.0000 scale=4.0000 norm=4.6278\n",
      "[iter 0] loss=1.4168 val_loss=0.0000 scale=8.0000 norm=9.3179\n",
      "[iter 0] loss=1.4326 val_loss=0.0000 scale=4.0000 norm=4.6599\n",
      "done with combination nr. 34\n",
      "[iter 0] loss=1.4079 val_loss=0.0000 scale=8.0000 norm=9.2688\n",
      "[iter 100] loss=0.5725 val_loss=0.0000 scale=2.0000 norm=0.6264\n",
      "[iter 200] loss=0.5509 val_loss=0.0000 scale=0.0039 norm=0.0011\n",
      "[iter 300] loss=0.5500 val_loss=0.0000 scale=0.0039 norm=0.0011\n",
      "[iter 0] loss=1.4282 val_loss=0.0000 scale=8.0000 norm=9.3514\n",
      "[iter 100] loss=0.5866 val_loss=0.0000 scale=1.0000 norm=0.3149\n",
      "[iter 200] loss=0.5651 val_loss=0.0000 scale=0.0039 norm=0.0011\n",
      "[iter 300] loss=0.5559 val_loss=0.0000 scale=0.0039 norm=0.0011\n",
      "[iter 0] loss=1.4314 val_loss=0.0000 scale=8.0000 norm=9.2741\n",
      "[iter 100] loss=0.5837 val_loss=0.0000 scale=2.0000 norm=0.6205\n",
      "[iter 200] loss=0.5644 val_loss=0.0000 scale=0.0312 norm=0.0090\n",
      "[iter 300] loss=0.5642 val_loss=0.0000 scale=0.0002 norm=0.0002\n",
      "done with combination nr. 35\n",
      "[iter 0] loss=1.4336 val_loss=0.0000 scale=4.0000 norm=4.6468\n",
      "[iter 100] loss=0.7938 val_loss=0.0000 scale=0.0002 norm=0.0004\n",
      "[iter 200] loss=0.7939 val_loss=0.0000 scale=0.0002 norm=0.0004\n",
      "[iter 0] loss=1.4150 val_loss=0.0000 scale=4.0000 norm=4.6404\n",
      "[iter 100] loss=0.7930 val_loss=0.0000 scale=0.0001 norm=0.0003\n",
      "[iter 200] loss=0.7931 val_loss=0.0000 scale=0.0001 norm=0.0003\n",
      "[iter 0] loss=1.4188 val_loss=0.0000 scale=8.0000 norm=9.3189\n",
      "[iter 100] loss=1.1419 val_loss=0.0000 scale=0.0000 norm=0.0003\n",
      "[iter 200] loss=1.1419 val_loss=0.0000 scale=0.0000 norm=0.0003\n",
      "done with combination nr. 36\n",
      "[iter 0] loss=1.4345 val_loss=0.0000 scale=4.0000 norm=4.6760\n",
      "[iter 100] loss=0.6968 val_loss=0.0000 scale=2.0000 norm=0.9505\n",
      "[iter 0] loss=1.4116 val_loss=0.0000 scale=8.0000 norm=9.2841\n",
      "[iter 100] loss=0.6703 val_loss=0.0000 scale=4.0000 norm=1.8671\n",
      "[iter 0] loss=1.4212 val_loss=0.0000 scale=8.0000 norm=9.2560\n",
      "[iter 100] loss=0.6847 val_loss=0.0000 scale=4.0000 norm=1.8661\n",
      "done with combination nr. 37\n",
      "[iter 0] loss=1.4251 val_loss=0.0000 scale=4.0000 norm=4.7130\n",
      "[iter 100] loss=nan val_loss=0.0000 scale=0.0002 norm=0.0003\n",
      "[iter 0] loss=1.4377 val_loss=0.0000 scale=4.0000 norm=4.6934\n",
      "[iter 100] loss=nan val_loss=0.0000 scale=0.0001 norm=0.0002\n",
      "[iter 0] loss=1.4519 val_loss=0.0000 scale=4.0000 norm=4.6850\n",
      "[iter 100] loss=nan val_loss=0.0000 scale=0.0001 norm=0.0002\n",
      "done with combination nr. 38\n",
      "[iter 0] loss=1.4368 val_loss=0.0000 scale=4.0000 norm=4.6619\n",
      "[iter 100] loss=0.6794 val_loss=0.0000 scale=4.0000 norm=1.7958\n",
      "[iter 200] loss=0.5992 val_loss=0.0000 scale=4.0000 norm=1.4752\n",
      "[iter 0] loss=1.4312 val_loss=0.0000 scale=4.0000 norm=4.6634\n",
      "[iter 100] loss=0.6658 val_loss=0.0000 scale=2.0000 norm=0.8893\n",
      "[iter 200] loss=0.6008 val_loss=0.0000 scale=2.0000 norm=0.7001\n",
      "[iter 0] loss=1.4220 val_loss=0.0000 scale=4.0000 norm=4.6488\n",
      "[iter 100] loss=0.6491 val_loss=0.0000 scale=4.0000 norm=1.6807\n",
      "[iter 200] loss=0.6107 val_loss=0.0000 scale=2.0000 norm=0.7213\n",
      "done with combination nr. 39\n",
      "[iter 0] loss=1.4542 val_loss=0.0000 scale=8.0000 norm=9.3682\n",
      "[iter 100] loss=0.5908 val_loss=0.0000 scale=2.0000 norm=0.6540\n",
      "[iter 0] loss=1.4302 val_loss=0.0000 scale=4.0000 norm=4.6369\n",
      "[iter 100] loss=0.5713 val_loss=0.0000 scale=2.0000 norm=0.6037\n",
      "[iter 0] loss=1.3957 val_loss=0.0000 scale=8.0000 norm=9.3743\n",
      "[iter 100] loss=0.5636 val_loss=0.0000 scale=2.0000 norm=0.6255\n",
      "done with combination nr. 40\n",
      "[iter 0] loss=1.4203 val_loss=0.0000 scale=4.0000 norm=4.6450\n",
      "[iter 0] loss=1.4152 val_loss=0.0000 scale=8.0000 norm=9.2590\n",
      "[iter 0] loss=1.4319 val_loss=0.0000 scale=4.0000 norm=4.6719\n",
      "done with combination nr. 41\n",
      "[iter 0] loss=1.4179 val_loss=0.0000 scale=2.0000 norm=2.2982\n",
      "[iter 100] loss=0.6977 val_loss=0.0000 scale=2.0000 norm=0.9778\n",
      "[iter 200] loss=0.5414 val_loss=0.0000 scale=2.0000 norm=0.5850\n",
      "[iter 0] loss=1.4192 val_loss=0.0000 scale=4.0000 norm=4.7486\n",
      "[iter 100] loss=0.7108 val_loss=0.0000 scale=2.0000 norm=0.9870\n",
      "[iter 200] loss=0.5509 val_loss=0.0000 scale=2.0000 norm=0.6268\n",
      "[iter 0] loss=1.4323 val_loss=0.0000 scale=4.0000 norm=4.6224\n",
      "[iter 100] loss=0.6973 val_loss=0.0000 scale=2.0000 norm=0.9731\n",
      "[iter 200] loss=0.5498 val_loss=0.0000 scale=2.0000 norm=0.5999\n",
      "done with combination nr. 42\n",
      "[iter 0] loss=1.4212 val_loss=0.0000 scale=4.0000 norm=4.6418\n",
      "[iter 0] loss=1.4155 val_loss=0.0000 scale=4.0000 norm=4.6070\n",
      "[iter 0] loss=1.4307 val_loss=0.0000 scale=4.0000 norm=4.6969\n",
      "done with combination nr. 43\n",
      "[iter 0] loss=1.4011 val_loss=0.0000 scale=4.0000 norm=4.6199\n",
      "[iter 100] loss=0.6727 val_loss=0.0000 scale=2.0000 norm=0.8884\n",
      "[iter 200] loss=0.6264 val_loss=0.0000 scale=2.0000 norm=0.7446\n",
      "[iter 300] loss=0.5993 val_loss=0.0000 scale=2.0000 norm=0.6752\n",
      "[iter 0] loss=1.4139 val_loss=0.0000 scale=4.0000 norm=4.6642\n",
      "[iter 100] loss=0.6165 val_loss=0.0000 scale=2.0000 norm=0.8464\n",
      "[iter 200] loss=0.6119 val_loss=0.0000 scale=2.0000 norm=0.7265\n",
      "[iter 300] loss=0.5976 val_loss=0.0000 scale=2.0000 norm=0.6513\n",
      "[iter 0] loss=1.4130 val_loss=0.0000 scale=4.0000 norm=4.6528\n",
      "[iter 100] loss=0.6752 val_loss=0.0000 scale=2.0000 norm=0.8827\n",
      "[iter 200] loss=0.5897 val_loss=0.0000 scale=2.0000 norm=0.7176\n",
      "[iter 300] loss=0.6003 val_loss=0.0000 scale=2.0000 norm=0.6746\n",
      "done with combination nr. 44\n",
      "[iter 0] loss=1.4200 val_loss=0.0000 scale=4.0000 norm=4.5917\n",
      "[iter 100] loss=0.4028 val_loss=0.0000 scale=2.0000 norm=0.3148\n",
      "[iter 200] loss=0.2868 val_loss=0.0000 scale=1.0000 norm=0.0992\n",
      "[iter 300] loss=0.2589 val_loss=0.0000 scale=2.0000 norm=0.1557\n",
      "[iter 0] loss=1.4240 val_loss=0.0000 scale=4.0000 norm=4.6890\n",
      "[iter 100] loss=0.4090 val_loss=0.0000 scale=1.0000 norm=0.1625\n",
      "[iter 200] loss=0.2962 val_loss=0.0000 scale=1.0000 norm=0.1047\n",
      "[iter 300] loss=0.2391 val_loss=0.0000 scale=1.0000 norm=0.0776\n",
      "[iter 0] loss=1.4490 val_loss=0.0000 scale=4.0000 norm=4.6965\n",
      "[iter 100] loss=0.4203 val_loss=0.0000 scale=2.0000 norm=0.3330\n",
      "[iter 200] loss=0.2971 val_loss=0.0000 scale=1.0000 norm=0.1051\n",
      "[iter 300] loss=0.2521 val_loss=0.0000 scale=1.0000 norm=0.0838\n",
      "done with combination nr. 45\n",
      "[iter 0] loss=1.4135 val_loss=0.0000 scale=8.0000 norm=9.3141\n",
      "[iter 100] loss=0.5151 val_loss=0.0000 scale=2.0000 norm=0.4986\n",
      "[iter 200] loss=0.5031 val_loss=0.0000 scale=1.0000 norm=0.2193\n",
      "[iter 0] loss=1.4086 val_loss=0.0000 scale=8.0000 norm=9.2011\n",
      "[iter 100] loss=0.5154 val_loss=0.0000 scale=1.0000 norm=0.2509\n",
      "[iter 200] loss=0.4832 val_loss=0.0000 scale=2.0000 norm=0.4312\n",
      "[iter 0] loss=1.4299 val_loss=0.0000 scale=4.0000 norm=4.7307\n",
      "[iter 100] loss=0.5297 val_loss=0.0000 scale=2.0000 norm=0.4998\n",
      "[iter 200] loss=0.4678 val_loss=0.0000 scale=1.0000 norm=0.2180\n",
      "done with combination nr. 46\n",
      "[iter 0] loss=1.4349 val_loss=0.0000 scale=8.0000 norm=9.3419\n",
      "[iter 0] loss=1.4209 val_loss=0.0000 scale=8.0000 norm=9.3649\n",
      "[iter 0] loss=1.4114 val_loss=0.0000 scale=8.0000 norm=9.1850\n",
      "done with combination nr. 47\n",
      "[iter 0] loss=1.4125 val_loss=0.0000 scale=2.0000 norm=2.2785\n",
      "[iter 0] loss=1.4105 val_loss=0.0000 scale=2.0000 norm=2.3380\n",
      "[iter 0] loss=1.4242 val_loss=0.0000 scale=2.0000 norm=2.3240\n",
      "done with combination nr. 48\n",
      "[iter 0] loss=1.4283 val_loss=0.0000 scale=4.0000 norm=4.6635\n",
      "[iter 100] loss=0.3230 val_loss=0.0000 scale=1.0000 norm=0.1119\n",
      "[iter 200] loss=0.2325 val_loss=0.0000 scale=2.0000 norm=0.1722\n",
      "[iter 0] loss=1.4191 val_loss=0.0000 scale=4.0000 norm=4.6506\n",
      "[iter 100] loss=0.3162 val_loss=0.0000 scale=1.0000 norm=0.1125\n",
      "[iter 200] loss=0.2088 val_loss=0.0000 scale=1.0000 norm=0.0694\n",
      "[iter 0] loss=1.4200 val_loss=0.0000 scale=4.0000 norm=4.6324\n",
      "[iter 100] loss=0.2837 val_loss=0.0000 scale=1.0000 norm=0.0984\n",
      "[iter 200] loss=0.1855 val_loss=0.0000 scale=1.0000 norm=0.0656\n",
      "done with combination nr. 49\n",
      "[iter 0] loss=1.4335 val_loss=0.0000 scale=4.0000 norm=4.6460\n",
      "[iter 100] loss=0.8115 val_loss=0.0000 scale=0.0002 norm=0.0004\n",
      "[iter 200] loss=0.8053 val_loss=0.0000 scale=0.0002 norm=0.0005\n",
      "[iter 0] loss=1.4230 val_loss=0.0000 scale=4.0000 norm=4.6808\n",
      "[iter 100] loss=0.7704 val_loss=0.0000 scale=0.0002 norm=0.0005\n",
      "[iter 200] loss=0.7705 val_loss=0.0000 scale=0.0002 norm=0.0005\n",
      "[iter 0] loss=1.4108 val_loss=0.0000 scale=8.0000 norm=9.2389\n",
      "[iter 100] loss=1.1419 val_loss=0.0000 scale=0.0000 norm=0.0003\n",
      "[iter 200] loss=1.1419 val_loss=0.0000 scale=0.0000 norm=0.0003\n",
      "Best hyperparameters: {'learning_rate': 0.05, 'minibatch_frac': 0.5, 'n_estimators': 200, 'Base': DecisionTreeRegressor(criterion='friedman_mse', max_depth=6)}\n",
      "Highest concordance index: 0.9244651845410813\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define your parameter grid here\n",
    "b1 = DecisionTreeRegressor(criterion='friedman_mse', max_depth=2)\n",
    "b2 = DecisionTreeRegressor(criterion='friedman_mse', max_depth=3)\n",
    "b3 = DecisionTreeRegressor(criterion='friedman_mse', max_depth=4)\n",
    "b4 = DecisionTreeRegressor(criterion='friedman_mse', max_depth=6)\n",
    "b5 = DecisionTreeRegressor(criterion='friedman_mse', max_depth=8)\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, .05, 1],\n",
    "    'minibatch_frac': [1.0, 0.5],\n",
    "    'n_estimators':  [75, 100, 200, 300, 400],\n",
    "    'Base':[b1, b2, b3, b4, b5]\n",
    "}\n",
    "\n",
    "X_NGB = train[features].reset_index()\n",
    "y_NGB = np.array(train.time_to_event.values.tolist())\n",
    "e_NGB = np.array(train.status.values.tolist())\n",
    "\n",
    "# Define your number of folds for cross-validation\n",
    "n_folds = 3\n",
    "\n",
    "# Define your number of random hyperparameter combinations to try\n",
    "n_combinations = 50\n",
    "\n",
    "# Define an empty list to store the results of each hyperparameter combination\n",
    "results = []\n",
    "\n",
    "# Loop through each hyperparameter combination\n",
    "for i in range(n_combinations):\n",
    "    \n",
    "    print(\"done with combination nr.\", i )\n",
    "    \n",
    "    # Sample a random combination of hyperparameters from the parameter grid\n",
    "    params = {\n",
    "        'learning_rate': random.choice(param_grid['learning_rate']),\n",
    "        'minibatch_frac': random.choice(param_grid['minibatch_frac']),\n",
    "        'n_estimators': random.choice(param_grid['n_estimators']),\n",
    "        'Base': random.choice(param_grid['Base'])\n",
    "    }\n",
    "    \n",
    "    # Define an empty list to store the results of each fold\n",
    "    fold_results = []\n",
    "    \n",
    "    # Split the data into folds for cross-validation\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "    for train_index, test_index in kf.split(X_NGB):\n",
    "        \n",
    "        # Split the data into training and testing sets for this fold\n",
    "        X_train_NGB, X_test_NGB = X_NGB.iloc[train_index], X_NGB.iloc[test_index]\n",
    "        y_train_NGB, y_test_NGB = y_NGB[train_index], y_NGB[test_index]\n",
    "        e_train_NGB, e_test_NGB = e_NGB[train_index], e_NGB[test_index]\n",
    "        \n",
    "        # Train your model with the current hyperparameters\n",
    "        model = NGBSurvival(Dist=LogNormal, **params)\n",
    "        model.fit(X_train_NGB, y_train_NGB, e_train_NGB)\n",
    "        \n",
    "        \n",
    "        train_CI = train.reset_index().iloc[train_index]\n",
    "        \n",
    "        y_train_CI = np.array(\n",
    "            list(zip(train_CI['status'], train_CI['time_to_event'])),\n",
    "            dtype=[('status', '?'), ('time_to_event', 'double')]\n",
    "        )\n",
    "        \n",
    "        \n",
    "        test_CI = train.reset_index().iloc[test_index]\n",
    "            \n",
    "        y_test_CI = np.array(\n",
    "            list(zip(test_CI['status'], test_CI['time_to_event'])),\n",
    "            dtype=[('status', '?'), ('time_to_event', 'double')]\n",
    "        )\n",
    "        \n",
    "        # Make predictions on the test set and calculate the accuracy\n",
    "        y_pred = model.pred_dist(X_test_NGB)\n",
    "        \n",
    "        try:\n",
    "            ci = concordance_index_ipcw(y_train_CI, y_test_CI, -y_pred.mean())[0]\n",
    "        except:\n",
    "            ci = 0\n",
    "            \n",
    "        \n",
    "        # Append the accuracy to the list of results for this fold\n",
    "        fold_results.append(ci)\n",
    "        \n",
    "\n",
    "    \n",
    "    # Calculate the average accuracy across all folds for this hyperparameter combination\n",
    "    avg_ci = sum(fold_results) / n_folds\n",
    "    \n",
    "    # Append the hyperparameter combination and its average accuracy to the list of results\n",
    "    results.append((params, avg_ci))\n",
    "    \n",
    "# Print the hyperparameter combination with the highest average accuracy\n",
    "best_params, best_ci = max(results, key=lambda x: x[1])\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "print(f\"Highest concordance index: {best_ci}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4a464eff-074c-441f-8b6f-adfe9af79b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NGB_survival(train, test, features, target, censoring, params):\n",
    "    clf = NGBSurvival(Dist=LogNormal, random_state = 42, **params)\n",
    "    Y_train = np.array(train.time_to_event.values.tolist())\n",
    "    E_train = np.array(train.status.values.tolist())\n",
    "    clf.fit(train[features], Y_train, E_train)\n",
    "    preds = clf.pred_dist(test[features])\n",
    "    return preds, clf\n",
    "\n",
    "def NGB_survival_result_wrapper(train, test, features, target, censoring, y_train, y_test, times, verbose, params):\n",
    "    model_output, model = NGB_survival(\n",
    "        train, test, features, target, censoring, params\n",
    "    )\n",
    "    \n",
    "    results = calc_metrics(y_train, y_test, -model_output.mean(), times)\n",
    "    return results, model, -model_output.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4e32db01-a2ee-4fc3-8b48-7a5ca8862d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=1.4263 val_loss=0.0000 scale=2.0000 norm=2.3360\n",
      "[iter 100] loss=0.4083 val_loss=0.0000 scale=2.0000 norm=0.3429\n",
      "({'cumulative_dynamic_auc': (array([0.98162915, 0.97214889, 0.97684449, 0.97351828, 0.97371318,\n",
      "       0.97642671, 0.97758091, 0.97980852, 0.97996872, 0.98058521,\n",
      "       0.98152646, 0.98237221, 0.98223945, 0.98253665, 0.98268761,\n",
      "       0.98335247, 0.98410531, 0.98319479, 0.98355727, 0.98046528,\n",
      "       0.97803362, 0.97890951, 0.97917655, 0.98084095, 0.98215743,\n",
      "       0.98329194, 0.9842116 , 0.9848041 , 0.9840475 , 0.98442952,\n",
      "       0.98451682, 0.98494622, 0.98554814, 0.98480003, 0.98387289,\n",
      "       0.98456765, 0.98487587, 0.98538845, 0.98570511, 0.98614576,\n",
      "       0.98649364, 0.98396468, 0.9827605 , 0.98386029, 0.98375093,\n",
      "       0.98323208, 0.98372718, 0.98341762, 0.98353179, 0.96944143,\n",
      "       0.96171707, 0.95976517, 0.96066848, 0.95903888, 0.96020078,\n",
      "       0.95963358, 0.96048108, 0.96132444, 0.9611078 , 0.95967165,\n",
      "       0.96067283, 0.95880133, 0.9557723 , 0.95635653, 0.95717547,\n",
      "       0.9575141 , 0.9592528 , 0.96018015, 0.96094943]), 0.9776911739975195), 'concordance_index_ipcw': (0.9289680351031268, 2390756, 102388, 0, 44876)}, NGBSurvival(Base=DecisionTreeRegressor(criterion='friedman_mse', max_depth=6),\n",
      "            Dist=<class 'ngboost.distns.utils.SurvivalDistnClass.<locals>.SurvivalDistn'>,\n",
      "            learning_rate=0.05, minibatch_frac=0.5, n_estimators=200,\n",
      "            random_state=RandomState(MT19937) at 0x29CED9540), array([ -15.62611936, -265.01863598, -308.00631464, ...,  -66.77435357,\n",
      "       -301.26952438, -314.68352363]))\n"
     ]
    }
   ],
   "source": [
    "results_NGB = NGB_survival_result_wrapper(\n",
    "    train,\n",
    "    test,\n",
    "    features,\n",
    "    target,\n",
    "    censoring,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    times_data,\n",
    "    False,\n",
    "    best_params\n",
    ")\n",
    "\n",
    "print(results_NGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "19500de4-cec8-4241-97ea-956e3b5e6605",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_NGB = results_NGB[1]\n",
    "NGB_results = results_NGB[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b65c3d20-5fbd-4dcd-83f1-aff1f2d9a46d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NGBSurvival' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [68]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_NGB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNGB_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_test.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, test[features])\n\u001b[1;32m      3\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_test.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, test[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_to_event\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NGBSurvival' object has no attribute 'save'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END max_depth=8, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=10;, score=0.916 total time=   5.7s\n",
      "[CV 3/3] END max_depth=8, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=10;, score=0.920 total time=   6.5s\n",
      "[CV 1/3] END max_depth=8, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=10;, score=0.918 total time=   6.7s\n"
     ]
    }
   ],
   "source": [
    "model_NGB.save('NGB_model.h5')\n",
    "np.save('x_test.npy', test[features])\n",
    "np.save('y_test.npy', test[['time_to_event', 'status']])\n",
    "np.save('preds.npy', NGB_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
